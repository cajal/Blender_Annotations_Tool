{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####2 goals\n",
    "\"\"\"1)  create the final datajoint tables to store the final spine meshs in:\n",
    "    a. One table to store the segment data (linked to the components table)\n",
    "    b. One table to store the labeled mesh vertices and faces of the neurons (linked to CleanseMesh)\n",
    "2) Create the function that goes through and writes the CGAL library for all of the components\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pseduo code for function that goes through and writes the CGAL library for all of the components\n",
    "\n",
    "1) Recieve list of neurons to do and a flag that when set will look to\n",
    "    another table for the clustering parameter for each neuron\n",
    "2) Pull down the neurons mesh data from cleansedMesh\n",
    "3) Pull down all the components with the neuron ID that have size > 100\n",
    "4) For each component:\n",
    "5) Generate the off file:\n",
    "    ---------------Way I do it in the blender file----------------\n",
    "    In load_Neuron_automatic_spine, download whole mesh\n",
    "    a. Before create the mesh object send faces and verts to filter_verts_and_faces\n",
    "    b. filter_verts_and_faces:\n",
    "        downloads the indexes for the compartment\n",
    "        Only saves off the verts that are mentioned in the indexes\n",
    "        Only saves off the faces that are mentioned in the indexes\n",
    "            returns them\n",
    "    c. builds the off file by:\n",
    "        Finding the faces that have all indices included in the verts list\n",
    "        finish with the write_Part_Neuron_Off_file\n",
    "    -------------------\n",
    "    1. create the file name string: \"neuron_\" + str(segment_id) + \"_\" + str(compartment_type_name) + \"_\" + str(found_component_index)\n",
    "    2. get the number of indices and faces\n",
    "    3. Open them and write them to the file\n",
    "    4. For the vertices:\n",
    "        For each index in the vertices blob of the components table, \n",
    "         write the coordinates in the index location of the Cleansed mesh table\n",
    "            while creating a lookup dictionary where it has old_vert_index:new_index  (vert_lookup)\n",
    "    5. For the faces:\n",
    "        For each index in the faces blob of the components table,\n",
    "             Get the new index by (vert_lookup) and save to list\n",
    "        Write the list to the file\n",
    "    \n",
    "    Call the CGAL function to generate the labels\n",
    "    String calculat the CGAL file name and the CGAL SDF value \n",
    "    Write the two lists to the datajoint table (linked to components)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "#from cloudvolume import CloudVolume\n",
    "#from collections import Counter\n",
    "#from funconnect import ta3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the address and the username\n",
    "dj.config['database.host'] = '10.28.0.34'\n",
    "dj.config['database.user'] = 'celiib'\n",
    "dj.config['database.password'] = 'newceliipass'\n",
    "dj.config['safemode']=True\n",
    "dj.config[\"display.limit\"] = 400\n",
    "\n",
    "\n",
    "# user: celiib\n",
    "# pass: newceliipass\n",
    "# host: at-database.ad.bcm.edu\n",
    "# schemas: microns_% and celiib_%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting celiib@10.28.0.34:3306\n"
     ]
    }
   ],
   "source": [
    "schema = dj.schema('microns_ta3')\n",
    "ta3 = dj.create_virtual_module('ta3', 'microns_ta3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2178"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ta3.ComponentLabel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "def generate_component_off_file(neuron_ID, compartment_type, component_id, n_vertex_indices, n_triangle_indices, \n",
    "                                vertex_indices, triangle_indices,vertices, triangles):\n",
    "    \n",
    "    #get the current file location\n",
    "    file_loc = pathlib.Path.cwd()\n",
    "    filename = \"neuron_\" + str(neuron_ID) + \"_\" + str(compartment_type) + \"_\" + str(component_id)\n",
    "    path_and_filename = file_loc / filename\n",
    "    \n",
    "    #open the file and start writing to it    \n",
    "    f = open(str(path_and_filename) + \".off\", \"w\")\n",
    "    f.write(\"OFF\\n\")\n",
    "    f.write(str(n_vertex_indices) + \" \" + str(n_triangle_indices) + \" 0\\n\" )\n",
    "    \n",
    "    #start writing all of the vertices\n",
    "    \"\"\"\n",
    "        4. For the vertices:\n",
    "        For each index in the vertices blob of the components table, \n",
    "         write the coordinates in the index location of the Cleansed mesh table\n",
    "            while creating a lookup dictionary where it has old_vert_index:new_index  (vert_lookup)\n",
    "    \"\"\"       \n",
    "    verts_lookup = {}\n",
    "    for i, vin in enumerate(vertex_indices):\n",
    "        #get the coordinates of the vertex\n",
    "        coordinates = vertices[vin]\n",
    "        #write the coordinates to the off file\n",
    "        f.write(str(coordinates[0]) + \" \" + str(coordinates[1]) + \" \" + str(coordinates[2])+\"\\n\")\n",
    "        #create lookup dictionary for vertices\n",
    "        verts_lookup[vin] = i\n",
    "    \n",
    "    \"\"\"    5. For the faces:\n",
    "        For each index in the faces blob of the components table,\n",
    "             Get the new index by (vert_lookup) and save to list\n",
    "        Write the list to the file\"\"\"\n",
    "    for i,fac in enumerate(triangle_indices):\n",
    "        verts_in_fac = triangles[fac]\n",
    "        #write the verties to the off file\n",
    "        f.write(\"3 \" + str(verts_lookup[verts_in_fac[0]]) + \" \" + str(verts_lookup[verts_in_fac[1]]) + \" \" + str(verts_lookup[verts_in_fac[2]])+\"\\n\")\n",
    "        \n",
    "    \n",
    "    print(\"Done making OFF file \" + str(filename))\n",
    "    #return the name of the off file you created and the location\n",
    "    return str(path_and_filename),str(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the neuron names from the component table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################THE ONE WE ARE USING\n",
    "import cgal_Segmentation_Module as csm\n",
    "import csv\n",
    "import decimal\n",
    "import time\n",
    "import os\n",
    "\n",
    "@schema\n",
    "class ComponentAutoSegment(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    # creates the labels for the mesh table\n",
    "    -> ta3.Compartment.Component\n",
    "    clusters     : tinyint unsigned  #what the clustering parameter was set to\n",
    "    smoothness   : decimal(3,2)             #what the smoothness parameter was set to, number betwee 0 and 1\n",
    "    ---\n",
    "    n_triangles  : int unsigned # number of faces\n",
    "    seg_group    : longblob     # group segmentation ID's for faces from automatic CGAL segmentation\n",
    "    sdf          : longblob     #  width values for faces from from automatic CGAL segmentation\n",
    "    median_sdf   : decimal(6,5) # the median width value for the sdf values\n",
    "    mean_sdf     : decimal(6,5) #the mean width value for the sdf values\n",
    "    third_q      : decimal(6,5) #the upper quartile for the mean width values\n",
    "    ninety_perc  : decimal(6,5) #the 90th percentile for the mean width values\n",
    "    time_updated : timestamp    # the time at which the segmentation was performed\n",
    "   \n",
    "    \n",
    "   \"\"\"\n",
    "    \n",
    "    key_source = ta3.Compartment.Component & 'n_triangle_indices>100' & [dict(compartment_type=comp) for comp in ['Basal', 'Apical', 'Oblique', 'Dendrite']]\n",
    "    \n",
    "    whole_neuron_dicts = dict()\n",
    "    \n",
    "    def make(self, key):\n",
    "        #key passed to function is just dictionary with the following attributes\n",
    "        \"\"\"segmentation\n",
    "        segment_id\n",
    "        decimation_ratio\n",
    "        compartment_type\n",
    "        component_index\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        clusters_default = 18\n",
    "        smoothness = 0.04\n",
    "\n",
    "        Apical_Basal_Oblique_default = [12,16]\n",
    "        basal_big = [16,18]\n",
    "\n",
    "        neuron_ID = key[\"segment_id\"]\n",
    "        component = (ta3.Compartment.Component & key).fetch1()        \n",
    "\n",
    "        component_id = component[\"component_index\"]\n",
    "        compartment_type = component[\"compartment_type\"]\n",
    "        component_size = int(component[\"n_triangle_indices\"])\n",
    "\n",
    "        print(\"component_size = \" + str(component_size))\n",
    "\n",
    "        if (compartment_type == \"Basal\") & (component_size > 160000):\n",
    "            cluster_list = basal_big\n",
    "        else:\n",
    "            cluster_list = Apical_Basal_Oblique_default\n",
    "\n",
    "\n",
    "        for clusters in cluster_list:\n",
    "            smoothness = 0.04\n",
    "            print(str(component[\"segment_id\"]) + \" type:\" + str(component[\"compartment_type\"]) \n",
    "                      + \" index:\" + str(component[\"component_index\"]) + \" cluster:\" + str(clusters) \n",
    "                  + \" smoothness:\" + str(smoothness))\n",
    "\n",
    "            #generate the off file for each component\n",
    "            #what need to send them:\n",
    "            \"\"\"----From cleansed Mesh---\n",
    "            vertices\n",
    "            triangles\n",
    "            ----From component table--\n",
    "            n_vertex_indices\n",
    "            n_triangle_indices\n",
    "            vertex_indices\n",
    "            triangle_indices\"\"\"\n",
    "            \n",
    "            if key['segment_id'] not in self.whole_neuron_dicts:\n",
    "                self.whole_neuron_dicts[key['segment_id']] = (ta3.CleansedMesh & 'decimation_ratio=0.35' & dict(segment_id=key['segment_id'])).fetch1()\n",
    "            \n",
    "            path_and_filename, off_file_name = generate_component_off_file(neuron_ID, compartment_type, component_id,\n",
    "                                        component[\"n_vertex_indices\"],\n",
    "                                        component[\"n_triangle_indices\"],\n",
    "                                        component[\"vertex_indices\"],\n",
    "                                        component[\"triangle_indices\"],\n",
    "                                        self.whole_neuron_dicts[key['segment_id']][\"vertices\"],\n",
    "                                        self.whole_neuron_dicts[key['segment_id']][\"triangles\"])\n",
    "            \n",
    "            print(len(component['vertex_indices']), len(component['triangle_indices']))\n",
    "            \n",
    "            #will have generated the component file by now so now need to run the segmentation\n",
    "            csm.cgal_segmentation(path_and_filename,clusters,smoothness)\n",
    "\n",
    "            #generate the name of the files\n",
    "            cgal_file_name = path_and_filename + \"-cgal_\" + str(clusters) + \"_\"+str(smoothness)\n",
    "            group_csv_cgal_file = cgal_file_name + \".csv\"\n",
    "            sdf_csv_file_name = cgal_file_name+\"_sdf.csv\"\n",
    "\n",
    "\n",
    "            with open(group_csv_cgal_file) as f:\n",
    "              reader = csv.reader(f)\n",
    "              your_list = list(reader)\n",
    "            group_list = []\n",
    "            for item in your_list:\n",
    "                group_list.append(int(item[0]))\n",
    "\n",
    "            with open(sdf_csv_file_name) as f:\n",
    "              reader = csv.reader(f)\n",
    "              your_list = list(reader)\n",
    "            sdf_list = []\n",
    "            for item in your_list:\n",
    "                sdf_list.append(float(item[0]))\n",
    "\n",
    "            #print(group_list)\n",
    "            #print(sdf_list)\n",
    "\n",
    "            #now write them to the datajoint table  \n",
    "            #table columns for ComponentAutoSegmentation: segmentation, segment_id, decimation_ratio, compartment_type, component_index, seg_group, sdf\n",
    "            comp_dict = dict(key,\n",
    "                                clusters=clusters,\n",
    "                                smoothness=smoothness,\n",
    "                                n_triangles=component[\"n_triangle_indices\"],\n",
    "                                seg_group=group_list,\n",
    "                                sdf=sdf_list,\n",
    "                                median_sdf=np.median(sdf_list),\n",
    "                                mean_sdf=np.mean(sdf_list),\n",
    "                                third_q=np.percentile(sdf_list, 75),\n",
    "                                ninety_perc=np.percentile(sdf_list, 90),\n",
    "                                time_updated=str(datetime.datetime.now())[0:19])\n",
    "\n",
    "            self.insert1(comp_dict)\n",
    "\n",
    "            #then go and erase all of the files used: the sdf files, \n",
    "            real_off_file_name = path_and_filename + \".off\"\n",
    "\n",
    "            files_to_delete = [group_csv_cgal_file,sdf_csv_file_name,real_off_file_name]\n",
    "            for fl in files_to_delete:\n",
    "                if os.path.exists(fl):\n",
    "                    os.remove(fl)\n",
    "                else:\n",
    "                    print(fl + \" file does not exist\")\n",
    "\n",
    "        print(\"finished\")\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "component_size = 182054\n",
      "50467565 type:Basal index:2 cluster:16 smoothness:0.04\n",
      "Done making OFF file neuron_50467565_Basal_2\n",
      "91046 182054\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/notebooks/shared/ActuallyImportant/SpineSegmentation[Brendan]/neuron_50467565_Basal_2-cgal_16_0.04.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-7c27e22378f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mComponentAutoSegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#reserve_jobs=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/datajoint/autopopulate.py\u001b[0m in \u001b[0;36mpopulate\u001b[0;34m(self, suppress_errors, return_exception_objects, reserve_jobs, order, limit, max_calls, display_progress, *restrictions)\u001b[0m\n\u001b[1;32m    150\u001b[0m                     \u001b[0mcall_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                         \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-5b0328347d76>\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_csv_cgal_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m               \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m               \u001b[0myour_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/notebooks/shared/ActuallyImportant/SpineSegmentation[Brendan]/neuron_50467565_Basal_2-cgal_16_0.04.csv'"
     ]
    }
   ],
   "source": [
    "ComponentAutoSegment.populate()#reserve_jobs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2178"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ComponentAutoSegment())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema.jobs.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'ta3' has no attribute 'ComponentLabel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9455d6daa4e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mta3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mComponentLabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'ta3' has no attribute 'ComponentLabel'"
     ]
    }
   ],
   "source": [
    "ta3.ComponentLabel().drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######table to write the labels from blender\n",
    "@schema\n",
    "class ComponentLabels(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    # creates the labels for the mesh table\n",
    "    -> ta3.ComponentAutoSegment\n",
    "    time_updated      :timestamp    # the time at which the component labels were updated\n",
    "    ---\n",
    "    n_vertices        :int unsigned #number of vertices in component\n",
    "    n_triangles       :int unsigned #number of faces in component\n",
    "    labeled_vertices  :longblob     #indicate which vertices are spine,spine_head,spine_neck otherwise 0\n",
    "    labeled_triangles :longblob     #indicate which faces are spine,spine_head,spine_neck otherwise 0\n",
    "    n_heads           :int unsigned #totals the number of heads after classification, helps for optimization\n",
    "    used_version      :tinyint      #whether this component is used in the final labels or not, 0 no, 1 yes\n",
    "    \n",
    "   \"\"\"\n",
    "    \n",
    "    def make(self, key):\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
